# check_split.py

import os
from document_loader import DocumentLoader
from text_splitter import TextSplitter
from config import DATA_DIR

# --- é…ç½®å‚æ•° ---
# å»ºè®®ä¸º TXT å’Œ DOCX è®¾ç½®ä¸€ä¸ªåˆç†çš„å—å¤§å°å’Œé‡å é‡
CHUNK_SIZE = 100 
CHUNK_OVERLAP = 20 
# --------------

def check_data_processing_pipeline():
    """
    æ£€æŸ¥æ–‡æ¡£åŠ è½½å’Œæ–‡æœ¬åˆ‡åˆ†æ¨¡å—çš„æ•´åˆè¿è¡Œæƒ…å†µã€‚
    """
    print("--- ğŸ› ï¸ RAG æ•°æ®å¤„ç†æµæ°´çº¿ç¬¬ä¸€é˜¶æ®µè‡ªæ£€å¼€å§‹ ---")
    
    if not os.path.exists(DATA_DIR):
        print(f"âŒ é”™è¯¯ï¼šæ•°æ®ç›®å½• {DATA_DIR} ä¸å­˜åœ¨ã€‚è¯·åˆ›å»ºè¯¥ç›®å½•å¹¶æ”¾å…¥æµ‹è¯•æ–‡ä»¶ã€‚")
        return

    # 1. å®ä¾‹åŒ–åŠ è½½å™¨å’Œåˆ‡åˆ†å™¨
    loader = DocumentLoader(data_dir=DATA_DIR)
    splitter = TextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)

    # 2. æ–‡æ¡£åŠ è½½é˜¶æ®µ (Load)
    print("\n## 1. æ–‡æ¡£åŠ è½½ (Document Loading)")
    
    # load_all_documents æ–¹æ³•ä¼šéå† DATA_DIR å¹¶åŠ è½½æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
    initial_documents = loader.load_all_documents()

    if not initial_documents:
        print("âŒ æ–‡æ¡£åŠ è½½å¤±è´¥æˆ–ç›®å½•ä¸­æ²¡æœ‰æ”¯æŒçš„æ–‡ä»¶ã€‚è¯·æ£€æŸ¥ data/ ç›®å½•å’Œæ–‡ä»¶æ ¼å¼ã€‚")
        return

    # ç»Ÿè®¡åŠ è½½ç»“æœ
    pdf_count = sum(1 for doc in initial_documents if doc["filetype"] == ".pdf")
    pptx_count = sum(1 for doc in initial_documents if doc["filetype"] == ".pptx")
    docx_count = sum(1 for doc in initial_documents if doc["filetype"] == ".docx")
    txt_count = sum(1 for doc in initial_documents if doc["filetype"] == ".txt")
    
    print(f"\nâœ… æ–‡æ¡£åŠ è½½å®Œæˆï¼å…±åŠ è½½ {len(initial_documents)} ä¸ªåˆå§‹å— (é¡µ/å¹»ç¯ç‰‡/æ•´ä½“æ–‡æ¡£)ã€‚")
    print(f"   - PDF é¡µæ•°/å—æ•°: {pdf_count}")
    print(f"   - PPTX å¹»ç¯ç‰‡æ•°/å—æ•°: {pptx_count}")
    print(f"   - DOCX æ–‡æ¡£æ•°: {docx_count}")
    print(f"   - TXT æ–‡æ¡£æ•°: {txt_count}")
    
    # 3. æ–‡æœ¬åˆ‡åˆ†é˜¶æ®µ (Split)
    print("\n## 2. æ–‡æœ¬åˆ‡åˆ† (Text Splitting)")
    
    # split_documents ä¼šå¯¹ DOCX/TXT è¿›è¡Œåˆ‡åˆ†ï¼Œå¯¹ PDF/PPTX ä¿æŒåŸæ ·
    final_chunks = splitter.split_documents(initial_documents)
    
    if not final_chunks:
        print("âŒ æ–‡æœ¬åˆ‡åˆ†å¤±è´¥ã€‚")
        return

    # 4. ç»“æœæ£€éªŒä¸é¢„è§ˆ
    print(f"\nâœ… æ–‡æœ¬åˆ‡åˆ†å®Œæˆï¼æœ€ç»ˆç”Ÿæˆ {len(final_chunks)} ä¸ª Chunkã€‚")
    print(f"   (ä½¿ç”¨å‚æ•°: chunk_size={CHUNK_SIZE}, chunk_overlap={CHUNK_OVERLAP})")
    
    # æ‰“å°å‰ 5 ä¸ª Chunk çš„é¢„è§ˆ
    print("\n### æœ€ç»ˆ Chunk ç»“æœé¢„è§ˆ (å‰ 5 ä¸ª):")
    sample_count = min(5, len(final_chunks))
    
    for i in range(sample_count):
        chunk = final_chunks[i]
        content_preview = chunk["content"].replace('\n', ' ') + '...'
        
        # æå–å…ƒæ•°æ®ä¿¡æ¯
        filename = chunk['filename']
        page_num = chunk['page_number']
        chunk_id = chunk['chunk_id']
        
        source_info = f"æ¥æº: {filename}"
        if chunk['filetype'] in ['.pdf', '.pptx']:
            source_info += f", é¡µ/å¹»ç¯ç‰‡: {page_num}"
        elif chunk['filetype'] in ['.docx', '.txt']:
            source_info += f", å— ID: {chunk_id}"
            
        print(f"\n[{i+1}] {source_info} (é•¿åº¦: {len(chunk['content'])}):")
        print(f"    å†…å®¹é¢„è§ˆ: {content_preview}")

    print("\n--- RAG æ•°æ®å¤„ç†æµæ°´çº¿ç¬¬ä¸€é˜¶æ®µè‡ªæ£€å®Œæˆ ---")


if __name__ == "__main__":
    check_data_processing_pipeline()