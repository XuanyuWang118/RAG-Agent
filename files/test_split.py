# check_split.py

import os
from document_loader import DocumentLoader
from text_splitter import TextSplitter
from config import DATA_DIR

# --- é…ç½®å‚æ•° ---
# å»ºè®®ä¸º TXT å’Œ DOCX è®¾ç½®ä¸€ä¸ªåˆç†çš„å—å¤§å°å’Œé‡å é‡
CHUNK_SIZE = 100 
CHUNK_OVERLAP = 20 
# --------------

def check_data_processing_pipeline():
    """
    æ£€æŸ¥æ–‡æ¡£åŠ è½½å’Œæ–‡æœ¬åˆ‡åˆ†æ¨¡å—çš„æ•´åˆè¿è¡Œæƒ…å†µï¼Œå¹¶éªŒè¯å›¾ç‰‡æ–‡æœ¬åŒ–ç»“æœã€‚
    """
    print("--- ğŸ› ï¸ RAG æ•°æ®å¤„ç†æµæ°´çº¿ç¬¬ä¸€é˜¶æ®µè‡ªæ£€å¼€å§‹ (å¤šæ¨¡æ€éªŒè¯) ---")
    
    if not os.path.exists(DATA_DIR):
        print(f"âŒ é”™è¯¯ï¼šæ•°æ®ç›®å½• {DATA_DIR} ä¸å­˜åœ¨ã€‚è¯·åˆ›å»ºè¯¥ç›®å½•å¹¶æ”¾å…¥æµ‹è¯•æ–‡ä»¶ã€‚")
        return

    # 1. å®ä¾‹åŒ–åŠ è½½å™¨å’Œåˆ‡åˆ†å™¨
    # ğŸš¨ æ³¨æ„ï¼šDocumentLoader é»˜è®¤åˆå§‹åŒ–æ—¶ä¼šåˆ›å»º ./images_extracted ç›®å½•
    loader = DocumentLoader(data_dir=DATA_DIR)
    # ğŸš¨ æ³¨æ„ï¼šTextSplitter åˆå§‹åŒ–æ—¶ä¼šå®ä¾‹åŒ– ImageProcessor
    splitter = TextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)

    # 2. æ–‡æ¡£åŠ è½½é˜¶æ®µ (Load)
    print("\n## 1. æ–‡æ¡£åŠ è½½ (Document Loading)")
    initial_documents = loader.load_all_documents()

    if not initial_documents:
        print("âŒ æ–‡æ¡£åŠ è½½å¤±è´¥æˆ–ç›®å½•ä¸­æ²¡æœ‰æ”¯æŒçš„æ–‡ä»¶ã€‚")
        return

    # ç»Ÿè®¡åŠ è½½ç»“æœå’Œå›¾ç‰‡å…ƒæ•°æ®
    pdf_count = sum(1 for doc in initial_documents if doc["filetype"] == ".pdf")
    pptx_count = sum(1 for doc in initial_documents if doc["filetype"] == ".pptx")
    total_image_meta = sum(len(doc.get("images", [])) for doc in initial_documents)

    print(f"\nâœ… æ–‡æ¡£åŠ è½½å®Œæˆï¼å…±åŠ è½½ {len(initial_documents)} ä¸ªåˆå§‹å—ã€‚")
    print(f" Â  - PDF/PPTX é¡µæ•°: {pdf_count + pptx_count}")
    print(f" Â  - ğŸ–¼ï¸ åˆæ­¥æå–åˆ°çš„å›¾ç‰‡å…ƒæ•°æ®æ€»æ•°: {total_image_meta} å¼ ")
    
    # 3. æ–‡æœ¬åˆ‡åˆ†ä¸å›¾ç‰‡æ–‡æœ¬åŒ–é˜¶æ®µ (Split & Process)
    print("\n## 2. æ–‡æœ¬åˆ‡åˆ†ä¸å›¾ç‰‡æ–‡æœ¬åŒ– (Multimodal Processing)")
    
    # ğŸš¨ è¿™ä¸€æ­¥å°†è°ƒç”¨ ImageProcessor å¯¹å›¾ç‰‡è¿›è¡Œ OCR/MLLM æ–‡æœ¬åŒ–ï¼Œå¹¶å°†ç»“æœè¿½åŠ åˆ° content å­—æ®µ
    final_chunks = splitter.split_documents(initial_documents)
    
    if not final_chunks:
        print("âŒ æ–‡æœ¬åˆ‡åˆ†æˆ–å›¾ç‰‡å¤„ç†å¤±è´¥ã€‚")
        return

    # 4. ç»“æœæ£€éªŒä¸é¢„è§ˆ
    print(f"\nâœ… æ–‡æœ¬åˆ‡åˆ†å®Œæˆï¼æœ€ç»ˆç”Ÿæˆ {len(final_chunks)} ä¸ª Chunkã€‚")
    
    # æ‰“å°å‰ 5 ä¸ª Chunk çš„é¢„è§ˆ
    print("\n### æœ€ç»ˆ Chunk ç»“æœé¢„è§ˆ (å‰ 5 ä¸ª Chunk çš„å†…å®¹å’Œå›¾ç‰‡åˆ†æéªŒè¯):")
    sample_count = min(20, len(final_chunks))
    
    for i in range(sample_count):
        chunk = final_chunks[i]
        content = chunk["content"]
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡åˆ†æçš„æ ‡è®°
        image_analysis_found = "--- å›¾åƒå†…å®¹åˆ†æ ---" in content or "--- å›¾ç‰‡ 1" in content
        
        # é•¿åº¦å’Œå†…å®¹é¢„è§ˆ
        content_preview = content.replace('\n', ' ')[:100] + '...'
        chunk_length = len(content)
        
        # æå–å…ƒæ•°æ®ä¿¡æ¯
        filename = chunk['filename']
        page_num = chunk['page_number']
        chunk_id = chunk['chunk_id']
        image_count = len(chunk.get('images', []))
        
        source_info = f"æ¥æº: {filename}"
        if chunk['filetype'] in ['.pdf', '.pptx']:
            source_info += f", é¡µ/å¹»ç¯ç‰‡: {page_num}"
        elif chunk['filetype'] in ['.docx', '.txt']:
            source_info += f", å— ID: {chunk_id}"
            
        
        print(f"\n[{i+1}] {source_info} (é•¿åº¦: {chunk_length}):")
        print(f" Â  Â å†…å®¹é¢„è§ˆ: {content_preview}")
        
        # é‡ç‚¹ï¼šå¤šæ¨¡æ€éªŒè¯
        if image_count > 0:
             # æ£€æŸ¥å›¾ç‰‡å…ƒæ•°æ®æ•°é‡å’Œå†…å®¹ä¸­æ˜¯å¦åŒ…å«åˆ†æç»“æœ
            status = f"âœ… å›¾ç‰‡å¤„ç†æˆåŠŸ ({image_count} å¼ å›¾ç‰‡å…ƒæ•°æ®)" if image_analysis_found else f"âš ï¸ å›¾ç‰‡å¤„ç†å¤±è´¥/æœªæ‰¾åˆ°åˆ†ææ ‡è®° ({image_count} å¼ å›¾ç‰‡å…ƒæ•°æ®)"
            print(f" Â  Â å¤šæ¨¡æ€éªŒè¯: {status}")
        else:
            print(f" Â  Â å¤šæ¨¡æ€éªŒè¯: ğŸ”  çº¯æ–‡æœ¬å—ï¼ˆæœªæå–åˆ°å›¾ç‰‡å…ƒæ•°æ®ï¼‰")


    print("\n--- RAG æ•°æ®å¤„ç†æµæ°´çº¿ç¬¬ä¸€é˜¶æ®µè‡ªæ£€å®Œæˆ ---")


if __name__ == "__main__":
    check_data_processing_pipeline()