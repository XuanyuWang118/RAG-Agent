import json
<<<<<<< HEAD
from typing import List, Dict, Optional, Tuple
=======
from typing import List, Dict, Optional, Tuple, Union
>>>>>>> 4ce53f2541db68d46cfaf9419f0cd50b06b35b63
from datetime import datetime

from openai import OpenAI

from config import (
    OPENAI_API_KEY,
    OPENAI_API_BASE,
    MODEL_NAME,
    TOP_K,
    DEFAULT_RETRIEVAL_STRATEGY, 
    ENABLE_ADVANCED_RAG,
)
from vector_store import VectorStore
from tools import ToolManager
from image_processor import ImageProcessor
<<<<<<< HEAD
from typing import Union
=======
>>>>>>> 4ce53f2541db68d46cfaf9419f0cd50b06b35b63


class RAGAgent:
    def __init__(
        self,
        model: str = MODEL_NAME,
    ):
        self.model = model

        self.client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_API_BASE)

        self.vector_store = VectorStore()
        
        # åˆå§‹åŒ–å›¾ç‰‡å¤„ç†å™¨å’Œå·¥å…·ç®¡ç†å™¨
        self.image_processor = ImageProcessor()
        self.tool_manager = ToolManager(rag_agent=self)
        
        # ã€æ–°å¢ã€‘ä¿å­˜ç­–ç•¥å¼€å…³çŠ¶æ€
        self.enable_advanced_rag = ENABLE_ADVANCED_RAG

<<<<<<< HEAD
        # åˆå§‹åŒ–å›¾ç‰‡å¤„ç†å™¨
        self.image_processor = ImageProcessor()

        # åˆå§‹åŒ–å·¥å…·ç®¡ç†å™¨ï¼ˆä¼ é€’è‡ªèº«å¼•ç”¨ä»¥æ”¯æŒå‡ºé¢˜å·¥å…·ï¼‰
        self.tool_manager = ToolManager(rag_agent=self)

        """
        TODO: å®ç°å¹¶è°ƒæ•´ç³»ç»Ÿæç¤ºè¯ï¼Œä½¿å…¶ç¬¦åˆè¯¾ç¨‹åŠ©æ•™çš„è§’è‰²å’Œå›ç­”ç­–ç•¥
        """
=======
>>>>>>> 4ce53f2541db68d46cfaf9419f0cd50b06b35b63
        self.system_prompt = """ä½ æ˜¯ä¸€ä½å‹å¥½ã€ä¸¥è°¨ä¸”ä¸“ä¸šçš„æ™ºèƒ½è¯¾ç¨‹åŠ©æ•™ã€‚
        ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„ã€è¯¾ç¨‹å†…å®¹ã€‘æ¥å›ç­”å­¦ç”Ÿçš„é—®é¢˜ã€‚

        **å›ç­”ç­–ç•¥ï¼ˆä¼˜å…ˆçº§é¡ºåºï¼‰ï¼š**
        1. **ä¼˜å…ˆä½¿ç”¨è¯¾ç¨‹å†…å®¹**ï¼šé¦–å…ˆåŸºäºã€è¯¾ç¨‹å†…å®¹ã€‘ä¸­çš„ä¿¡æ¯å›ç­”é—®é¢˜
        2. **è¡¥å……è”ç½‘æœç´¢**ï¼šå¦‚æœã€è¯¾ç¨‹å†…å®¹ã€‘ä¿¡æ¯ä¸è¶³æˆ–æ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œæ‰ä½¿ç”¨è”ç½‘æœç´¢è·å–è¡¥å……ä¿¡æ¯
        3. **ç›´æ¥å›ç­”**ï¼šå¯¹äºä¸æ¶‰åŠè¯¾ç¨‹çŸ¥è¯†çš„ä¸€èˆ¬æ€§é—®é¢˜ï¼ˆå¦‚æ—¶é—´ã€ç®€å•è®¡ç®—ï¼‰ï¼Œç›´æ¥å›ç­”æ— éœ€è¿½æº¯æ¥æº

        **å›¾ç‰‡è¾“å…¥è¯´æ˜ï¼š**
        - å½“ç”¨æˆ·æäº¤å›¾ç‰‡æ—¶ï¼Œç³»ç»Ÿä¼šå…ˆä½¿ç”¨AIè§†è§‰æ¨¡å‹åˆ†æå›¾ç‰‡å†…å®¹ï¼Œç”Ÿæˆæ–‡å­—æè¿°
        - å›¾ç‰‡æè¿°ä¼šåŒ…å«åœ¨æŸ¥è¯¢ä¸­ï¼Œå¸®åŠ©ä½ æ›´å¥½åœ°ç†è§£ç”¨æˆ·çš„é—®é¢˜
        - ä½ åº”è¯¥å°†å›¾ç‰‡æè¿°ä¸è¯¾ç¨‹å†…å®¹ç›¸ç»“åˆï¼Œæä¾›å‡†ç¡®ã€ä¸“ä¸šçš„è§£ç­”

        **æ™ºèƒ½å‡ºé¢˜åŠŸèƒ½ï¼š**
        - åœ¨é€‚å½“çš„æ—¶æœºï¼Œä½ å¯ä»¥ä¸»åŠ¨è¯¢é—®å­¦ç”Ÿæ˜¯å¦éœ€è¦ç”Ÿæˆä¹ é¢˜æ¥å·©å›ºçŸ¥è¯†ç‚¹
        - å½“å­¦ç”Ÿè¡¨è¾¾å­¦ä¹ éœ€æ±‚æˆ–å®ŒæˆæŸä¸ªçŸ¥è¯†ç‚¹è®²è§£åï¼Œä½ å¯ä»¥å»ºè®®ï¼š"éœ€è¦æˆ‘ä¸ºä½ ç”Ÿæˆä¸€äº›ç»ƒä¹ é¢˜æ¥å·©å›ºè¿™ä¸ªçŸ¥è¯†ç‚¹å—ï¼Ÿ"
        - å¦‚æœå­¦ç”ŸåŒæ„ï¼Œä½ å¯ä»¥è°ƒç”¨ `quiz_generation` å·¥å…·æ¥ç”Ÿæˆç›¸å…³ä¹ é¢˜
<<<<<<< HEAD
        - ä¹ é¢˜åº”è¯¥åŸºäºå½“å‰å¯¹è¯çš„ä¸»é¢˜ï¼Œéš¾åº¦é€‚ä¸­ï¼Œæœ‰è¯¦ç»†çš„è§£æè¯´æ˜
=======
>>>>>>> 4ce53f2541db68d46cfaf9419f0cd50b06b35b63
        - ç¤ºä¾‹è°ƒç”¨ï¼šquiz_generation(topic="è¯å‘é‡", difficulty="medium", question_type="multiple_choice", num_questions=3)
        - é‡è¦ï¼šè°ƒç”¨å·¥å…·åï¼Œé¢˜ç›®ä¼šè‡ªåŠ¨æ˜¾ç¤ºåœ¨ç”¨æˆ·ç•Œé¢ä¸­ï¼Œä½ ä¸éœ€è¦åœ¨å›å¤ä¸­é‡å¤åŒ…å«é¢˜ç›®å†…å®¹
        - ä½ çš„å›å¤åº”è¯¥ç®€æ´åœ°ç¡®è®¤é¢˜ç›®å·²ç”Ÿæˆï¼Œå¼•å¯¼ç”¨æˆ·æŸ¥çœ‹ç•Œé¢ç­”é¢˜

        å›ç­”è¦æ±‚ï¼š
        1. **åŸºäºäº‹å®**ï¼šæ‰€æœ‰å›ç­”å¿…é¡»ä¸¥æ ¼åŸºäºã€è¯¾ç¨‹å†…å®¹ã€‘æˆ–è€…è”ç½‘æœç´¢ä¸­æ£€ç´¢åˆ°çš„ä¿¡æ¯ã€‚
        2. **è¿½æº¯æ¥æº**ï¼šåœ¨å›ç­”ä¸­ä½¿ç”¨è¯¾ç¨‹å†…å®¹æ—¶ï¼Œå¿…é¡»åœ¨å¼€å¤´æˆ–æœ«å°¾æ ‡æ³¨ä¿¡æ¯æ¥æºï¼Œæ ¼å¼ä¸ºï¼š[æ¥æºï¼šæ–‡ä»¶åï¼Œé¡µç  X æˆ– å¹»ç¯ç‰‡ X] æˆ– [æ¥æºï¼šæ–‡ä»¶å]ï¼ˆè‹¥æ— é¡µç ï¼‰ã€‚å¦‚æœä½¿ç”¨äº†è”ç½‘æœç´¢ï¼Œæ ‡æ³¨ä¸ºï¼š[æ¥æºï¼šç½‘ç»œæœç´¢ç»“æœ]ã€‚å¦‚æœæœ‰å¤šä¸ªæ¥æºï¼Œè¯·åˆå¹¶æˆ–åˆ†åˆ«æ ‡æ³¨ã€‚
        3. **æ— æ³•å›ç­”**ï¼šå¦‚æœã€è¯¾ç¨‹å†…å®¹ã€‘å’Œè”ç½‘æœç´¢ä¸­éƒ½æ‰¾ä¸åˆ°è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”å­¦ç”Ÿçš„é—®é¢˜ï¼Œè¯·åŸºäºä½ è‡ªå·±çš„è®¤çŸ¥å›ç­”ï¼Œå¹¶åœ¨æœ€åå‘ŠçŸ¥å­¦ç”Ÿï¼š"æœªå¯»æ‰¾åˆ°ç›¸å…³è¯¾ç¨‹ææ–™ï¼Œå›ç­”ä»…ä¾›å‚è€ƒï¼Œè¯·æŸ¥é˜…æ•™ææˆ–è€…è¯¢é—®è€å¸ˆã€‚"
        4. **è¯­æ°”ä¸“ä¸š**ï¼šä¿æŒåŠ©æ•™çš„ä¸“ä¸šã€å‹å¥½å’Œæ¡ç†æ¸…æ™°çš„è¯­æ°”ã€‚
        """

    # def _construct_search_query(self, current_query: str, chat_history: Optional[List[Dict]] = None) -> str:
    #     """
    #     ã€æ–°å¢ã€‘ä½¿ç”¨å¯¹è¯å†å²æ¥æç‚¼æœç´¢å…³é”®è¯ï¼Œæå‡å¤šè½®æ£€ç´¢ç²¾åº¦ã€‚
    #     ä»…åœ¨ self.enable_advanced_rag å¼€å¯æ—¶ï¼Œæ‰æ‰§è¡Œå¤šè½®å¢å¼ºã€‚
    #     """
    #     if not self.enable_advanced_rag:
    #         return current_query
            
    #     # æ’é™¤åŒ…å«å›¾ç‰‡æè¿°çš„å¢å¼ºæŸ¥è¯¢ï¼Œé¿å…é‡å¤åµŒå¥—
    #     if current_query.startswith("ã€ç”¨æˆ·æäº¤çš„å›¾ç‰‡åˆ†æç»“æœã€‘"):
    #          return current_query

    #     if not chat_history or len(chat_history) < 2:
    #         return current_query
        
    #     # æå–æœ€è¿‘çš„é—®ç­”å¯¹
    #     last_exchange = chat_history[-2:]
        
    #     # æ„é€ ç”¨äº RAG æ£€ç´¢çš„æœ€ç»ˆæŸ¥è¯¢
    #     recent_context = f"æœ€è¿‘çš„é—®é¢˜ï¼š{last_exchange[0]['content']}ï¼Œæœ€è¿‘çš„å›ç­”ï¼š{last_exchange[1]['content']}ã€‚"
    #     return f"{recent_context} å­¦ç”Ÿçš„æ–°é—®é¢˜æ˜¯ï¼š{current_query}"
    def _construct_search_query(self, current_query: str, chat_history: Optional[List[Dict]] = None) -> str:
        """
        ã€ä¿®æ­£ã€‘ä½¿ç”¨å¯¹è¯å†å²æ¥æç‚¼æœç´¢å…³é”®è¯ï¼Œæå‡å¤šè½®æ£€ç´¢ç²¾åº¦ã€‚
        """
        if not self.enable_advanced_rag:
            return current_query
            
        # æ’é™¤åŒ…å«å›¾ç‰‡æè¿°çš„å¢å¼ºæŸ¥è¯¢ï¼Œé¿å…é‡å¤åµŒå¥—
        if current_query.startswith("ã€ç”¨æˆ·æäº¤çš„å›¾ç‰‡åˆ†æç»“æœã€‘"):
             return current_query

        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å†å²è®°å½•
        if not chat_history or len(chat_history) < 2:
            return current_query
        
        # æå–æœ€è¿‘çš„é—®ç­”å¯¹
        # éå†å†å²è®°å½•ï¼Œæ‰¾åˆ°æœ€æ–°çš„ User å’Œ Assistant æ¶ˆæ¯
        relevant_history = []
        for msg in reversed(chat_history):
            # ä»…è€ƒè™‘ user å’Œ assistant è§’è‰²
            if msg.get('role') in ['user', 'assistant'] and 'content' in msg:
                # æ’é™¤å·¥å…·è°ƒç”¨ç›¸å…³çš„ assistant æ¶ˆæ¯
                if msg.get('role') == 'assistant' and msg.get('content', '').startswith("ğŸ¯ å·²ç”Ÿæˆä¹ é¢˜"):
                     continue
                
                relevant_history.append(msg)
            if len(relevant_history) >= 2:
                break
        
        # å¦‚æœæ‰¾ä¸åˆ°æœ€æ–°çš„é—®ç­”å¯¹ï¼Œåˆ™è¿”å›åŸå§‹æŸ¥è¯¢
        if len(relevant_history) < 2:
            return current_query
        
        # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
        # relevant_history[0] æ˜¯æœ€æ–°çš„æ¶ˆæ¯
        # ç¡®ä¿é¡ºåºæ˜¯ [æœ€æ–°å›å¤ (Assistant), æœ€æ–°æé—® (User)]
        
        # LLM æç‚¼ Prompt
        context_for_llm = f"""
        ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢æç‚¼åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹å¯¹è¯å†å²æ¥å®Œå–„ç”¨æˆ·çš„æœ€æ–°æŸ¥è¯¢ï¼Œä»¥æ›´å¥½åœ°è¿›è¡ŒRAGæ£€ç´¢ã€‚
        
        å¯¹è¯å†å²:
        - ä¸Šä¸€æ¬¡å›å¤ï¼ˆåŠ©æ•™ï¼‰ï¼š"{relevant_history[0]['content']}"
        - ä¸Šä¸€æ¬¡æé—®ï¼ˆå­¦ç”Ÿï¼‰ï¼š"{relevant_history[1]['content']}"
        - ç”¨æˆ·çš„æœ€æ–°æé—®æ˜¯ï¼š"{current_query}"

        ä»»åŠ¡ï¼šè¯·æå–æˆ–é‡å†™ä¸€ä¸ª**ç²¾ç¡®ä¸”ç‹¬ç«‹**çš„æ£€ç´¢æŸ¥è¯¢ï¼ˆç”¨äºæœç´¢çŸ¥è¯†åº“ï¼‰ï¼Œè¯¥æŸ¥è¯¢åº”ç»“åˆå¯¹è¯å†å²ä¸­çš„æŒ‡ä»£å…³ç³»æˆ–çœç•¥ä¿¡æ¯ã€‚
        ä¾‹å¦‚ï¼Œå¦‚æœæœ€æ–°æé—®æ˜¯"å®ƒæœ‰ä»€ä¹ˆç¼ºç‚¹?"ï¼Œè€Œä¸Šä¸€æ¬¡æé—®æ˜¯"ä»€ä¹ˆæ˜¯Transformeræ¨¡å‹"ï¼Œé‚£ä¹ˆä½ åº”è¿”å›"Transformeræ¨¡å‹çš„ç¼ºç‚¹"ã€‚
        """
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": context_for_llm}],
                temperature=0.0, # ç¡®ä¿è¾“å‡ºç¨³å®š
                max_tokens=200
            )
            enhanced_query = response.choices[0].message.content.strip()
            # æ’é™¤å¼•å·ï¼Œé˜²æ­¢ JSON è§£æé—®é¢˜
            enhanced_query = enhanced_query.strip().replace('"', '') 
            
            print(f"ğŸ”„ å¤šè½®å¯¹è¯å¢å¼ºæŸ¥è¯¢: {enhanced_query}")
            return enhanced_query
        except Exception as e:
            print(f"âŒ å¤šè½®æŸ¥è¯¢å¢å¼ºå¤±è´¥ ({e})ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢ã€‚")
            return current_query

    def _analyze_query_type(self, query: str) -> str:
        """
        ã€æ–°å¢ã€‘ä½¿ç”¨ LLM åˆ†ææŸ¥è¯¢æ„å›¾å’Œç±»å‹ï¼Œä»¥å†³å®šæœ€ä½³æ£€ç´¢ç­–ç•¥ã€‚
        """
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ£€ç´¢ç­–ç•¥åˆ†æå™¨ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·æŸ¥è¯¢çš„æ€§è´¨å’Œæ„å›¾ï¼Œ
        åœ¨ä¸¥æ ¼é™å®šçš„ä¸‰ç§æ£€ç´¢ç­–ç•¥ä¸­ï¼Œé€‰æ‹©å¹¶è¿”å›æœ€ä¼˜åŒ–æ£€ç´¢ç»“æœçš„é‚£ä¸€ä¸ªã€‚
        åˆ†ææ—¶è¯·åŒæ—¶è€ƒè™‘æŸ¥è¯¢çš„**å…³é”®è¯ç¨€æœ‰åº¦**å’Œ**è¯­ä¹‰æŠ½è±¡åº¦**ã€‚

        --- ç­–ç•¥å®šä¹‰å’Œé€‰æ‹©æ ‡å‡† ---

        1. 'DENSE' (å¯†é›†æ£€ç´¢/å‘é‡æ£€ç´¢):
           - **é€‚ç”¨åœºæ™¯ï¼š** æŸ¥è¯¢æ¶‰åŠå®šä¹‰ã€åŸç†ã€æ¯”è¾ƒã€å…³ç³»ã€å¹¿ä¹‰æ¦‚å¿µæˆ–éœ€è¦æ·±åº¦è¯­ä¹‰ç†è§£ã€‚
           - **å…¸å‹ç‰¹å¾ï¼š** å¥å­ç»“æ„å®Œæ•´ï¼Œå…³é”®è¯æŠ½è±¡åº¦é«˜ã€‚

        2. 'BM25' (ç¨€ç–æ£€ç´¢/å…³é”®è¯æ£€ç´¢):
           - **é€‚ç”¨åœºæ™¯ï¼š** æŸ¥è¯¢åŒ…å«ç½•è§ã€ä¸“ä¸šã€æŠ€æœ¯æ€§åè¯ã€IDã€ä»£ç ç‰‡æ®µæˆ–ç‰¹å®šæ•°å­—ï¼Œä¸”è¦æ±‚ç²¾ç¡®çš„å­—é¢åŒ¹é…ã€‚
           - **å…¸å‹ç‰¹å¾ï¼š** å…³é”®è¯ç¨€æœ‰åº¦é«˜ã€‚

        3. 'HYBRID' (æ··åˆæ£€ç´¢/RRFèåˆ):
           - **é€‚ç”¨åœºæ™¯ï¼š** æŸ¥è¯¢æ¨¡ç³Šã€åŒæ—¶åŒ…å«æŠ½è±¡æ¦‚å¿µå’Œç¨€æœ‰å…³é”®è¯ï¼Œæˆ–è€…åœ¨å¤šè½®å¯¹è¯ä¸­ä½¿ç”¨äº†ä»£è¯ï¼ˆå¦‚â€œå®ƒâ€ã€â€œè¿™ä¸ªâ€ï¼‰è¿›è¡ŒæŒ‡ä»£ã€‚
           - **å…¸å‹ç‰¹å¾ï¼š** å…¼å…·è¯­ä¹‰å’Œå…³é”®è¯ç‰¹å¾ã€‚

        --- ä»»åŠ¡å’Œæ ¼å¼è¦æ±‚ ---

        ç”¨æˆ·æŸ¥è¯¢: "{query}"

        è¯·ä¸¥æ ¼ä»…è¿”å›ä»¥ä¸‹ä¸‰ç§å­—ç¬¦ä¸²ä¹‹ä¸€ï¼Œä¸æ·»åŠ ä»»ä½•è§£é‡Šã€æ ‡ç‚¹æˆ–å…¶ä»–æ–‡æœ¬ï¼š'HYBRID', 'BM25', 'DENSE'
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                max_tokens=10
            )
            # æ¸…ç†å’Œè§„èŒƒåŒ–è¾“å‡º
            return response.choices[0].message.content.strip().upper().replace('"', '')
        except Exception:
            # å¤±è´¥æ—¶é»˜è®¤ä½¿ç”¨ config ä¸­çš„ç­–ç•¥
            return DEFAULT_RETRIEVAL_STRATEGY

    def retrieve_context(
        self, query: str, chat_history: Optional[List[Dict]] = None, top_k: int = TOP_K
    ) -> Tuple[str, List[Dict]]:
        """
        ã€é‡æ„ã€‘å®ç°æ£€ç´¢ç­–ç•¥åˆ†æ´¾å™¨ (Strategy Dispatcher)ã€‚
        æ–°å¢é€»è¾‘ï¼šå¦‚æœ self.enable_advanced_rag ä¸º Falseï¼Œåˆ™å¼ºåˆ¶ä½¿ç”¨ DENSE ç­–ç•¥ã€‚
        """
        # 1. æ„é€ ç”¨äºæ£€ç´¢çš„å¢å¼ºæŸ¥è¯¢ (è¯¥å‡½æ•°å†…éƒ¨ä¼šæ ¹æ®å¼€å…³è¿”å›åŸå§‹æˆ–å¢å¼ºæŸ¥è¯¢)
        # æ³¨æ„ï¼šè¿™é‡Œå°† chat_history ä¼ é€’ç»™ _construct_search_query
        search_query = self._construct_search_query(query, chat_history)

        retrieved_docs = []
        
        # --- ç­–ç•¥å†³ç­–å¼€å§‹ ---
        if not self.enable_advanced_rag:
            # ã€é€€åŒ–é€»è¾‘ã€‘å¦‚æœé«˜çº§RAGå¼€å…³å…³é—­ï¼Œå¼ºåˆ¶é€€åŒ–åˆ°çº¯å‘é‡ï¼ˆDENSEï¼‰ç­–ç•¥ã€‚
            query_type = "DENSE"
            print("âš™ï¸ é«˜çº§RAGå¢å¼ºå·²å…³é—­ï¼Œå¼ºåˆ¶é€€åŒ–åˆ°çº¯å‘é‡å¯†é›†æ£€ç´¢ (DENSE) ç­–ç•¥ã€‚")
            
        else:
            # å¯ç”¨é«˜çº§ç­–ç•¥ï¼šæ‰§è¡Œ LLM ç­–ç•¥åˆ†æ
            query_type = self._analyze_query_type(search_query)
            print(f"âš™ï¸ é«˜çº§RAGå¢å¼ºå·²å¯ç”¨ | LLMåˆ†æç­–ç•¥: {query_type}")
            # 

        # 2. ç­–ç•¥åˆ†æ´¾å™¨ (Dispatching logic based on query_type)
        if query_type == 'DENSE':
            # æ¦‚å¿µä¸»å¯¼æˆ–é€€åŒ–ç­–ç•¥ï¼šçº¯å‘é‡æ£€ç´¢
            retrieved_docs = self.vector_store.search_dense(search_query, top_k=top_k)
            print("â¡ï¸ é‡‡ç”¨çº¯å‘é‡å¯†é›†æ£€ç´¢ (search_dense)")
            
        elif query_type == 'BM25':
            # å…³é”®è¯ä¸»å¯¼ï¼šçº¯ç¨€ç–æ£€ç´¢
            retrieved_docs = self.vector_store.search_bm25(search_query, top_k=top_k)
            print("â¡ï¸ é‡‡ç”¨çº¯ BM25 ç¨€ç–æ£€ç´¢ (search_bm25)")

        elif query_type == 'HYBRID': 
            # æ··åˆæ£€ç´¢ (å‡è®¾ self.vector_store.search æ˜¯ HYBRID å®ç°)
            retrieved_docs = self.vector_store.search(search_query, top_k=top_k)
            print("â¡ï¸ é‡‡ç”¨ RRF æ··åˆæ£€ç´¢ (search)")
        
        else:
            # LLMåˆ†æå¤±è´¥æ—¶çš„å…œåº•ç­–ç•¥
            if DEFAULT_RETRIEVAL_STRATEGY == "BM25":
                 retrieved_docs = self.vector_store.search_bm25(search_query, top_k=top_k)
            elif DEFAULT_RETRIEVAL_STRATEGY == "DENSE":
                 retrieved_docs = self.vector_store.search_dense(search_query, top_k=top_k)
            else:
                 retrieved_docs = self.vector_store.search(search_query, top_k=top_k)
            print(f"âš ï¸ LLM åˆ†æå¤±è´¥ï¼Œå›é€€åˆ° DEFAULT ç­–ç•¥: {DEFAULT_RETRIEVAL_STRATEGY}")
            
        # --- ç­–ç•¥å†³ç­–ç»“æŸ ---

        # 3. æ ¼å¼åŒ–æ£€ç´¢ç»“æœï¼ˆä¿æŒåŸé€»è¾‘ä¸å˜ï¼‰
        context_parts = []
        source_set = set()
        
        for doc in retrieved_docs:
            content = doc["content"]
            metadata = doc["metadata"]
            
            filename = metadata.get("filename", "æœªçŸ¥æ–‡ä»¶")
            page_number = metadata.get("page_number", 0)
            
            if page_number and page_number > 0:
                source_label = "é¡µç " if metadata.get("filetype") == ".pdf" else "å¹»ç¯ç‰‡"
                source_info = f"[æ¥æºï¼š{filename}, {source_label} {page_number}]"
                source_set.add(f"{filename}, {source_label} {page_number}")
            else:
                source_info = f"[æ¥æºï¼š{filename}]"
                source_set.add(filename)
                
            context_parts.append(f"{source_info}\n{content}\n---")

        context_string = "\n".join(context_parts)
        
        return context_string, retrieved_docs

    def generate_response(
        self,
        query: str,
        context: str,
        chat_history: Optional[List[Dict]] = None,
    ) -> str:
        """ç”Ÿæˆå›ç­”"""
        messages = [{"role": "system", "content": self.system_prompt}]

        if chat_history:
            messages.extend(chat_history)

        user_text = f"""
        è¯·åŸºäºä¸‹é¢çš„ã€è¯¾ç¨‹å†…å®¹ã€‘æ¥å›ç­”å­¦ç”Ÿçš„é—®é¢˜ã€‚è¯·ä¸¥æ ¼éµå¾ªç³»ç»Ÿæç¤ºè¯ä¸­çš„æ‰€æœ‰è¦æ±‚ã€‚

        **ä¼˜å…ˆçº§ç­–ç•¥ï¼š**
        1. é¦–å…ˆåŸºäºã€è¯¾ç¨‹å†…å®¹ã€‘å›ç­”é—®é¢˜
        2. åªæœ‰åœ¨ã€è¯¾ç¨‹å†…å®¹ã€‘ä¿¡æ¯ä¸è¶³æ—¶ï¼Œæ‰ä½¿ç”¨å·¥å…·è·å–è¡¥å……ä¿¡æ¯

        ã€è¯¾ç¨‹å†…å®¹ã€‘
        {context}

        {query}

        å¦‚æœã€è¯¾ç¨‹å†…å®¹ã€‘æ— æ³•æä¾›è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œä½ å¯ä»¥é€‰æ‹©ä½¿ç”¨æä¾›çš„å·¥å…·æœç´¢ç½‘ç»œä¿¡æ¯ã€è¿›è¡Œè®¡ç®—æˆ–è·å–å½“å‰æ—¶é—´ã€‚
        """

        messages.append({"role": "user", "content": user_text})
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                tools=self.tool_manager.get_tool_definitions(),
<<<<<<< HEAD
                tool_choice="auto",  # è®©AIè‡ªåŠ¨å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
=======
                tool_choice="auto",
>>>>>>> 4ce53f2541db68d46cfaf9419f0cd50b06b35b63
                temperature=0.7,
                max_tokens=1500
            )

            response_message = response.choices[0].message

<<<<<<< HEAD
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            if response_message.tool_calls:
                # æ‰§è¡Œå·¥å…·è°ƒç”¨
                tool_results = self._execute_tool_calls(response_message.tool_calls)

                # å°†å·¥å…·è°ƒç”¨ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²
=======
            if response_message.tool_calls:
                tool_results = self._execute_tool_calls(response_message.tool_calls)

>>>>>>> 4ce53f2541db68d46cfaf9419f0cd50b06b35b63
                messages.append(response_message)
                for tool_result in tool_results:
                    messages.append(tool_result)

<<<<<<< HEAD
                # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼šåŸºäºå·¥å…·ç»“æœç”Ÿæˆæœ€ç»ˆå›ç­”
=======
>>>>>>> 4ce53f2541db68d46cfaf9419f0cd50b06b35b63
                final_response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=0.7,
                    max_tokens=1500
                )

                return final_response.choices[0].message.content
            else:
<<<<<<< HEAD
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç›´æ¥è¿”å›ç»“æœ
=======
>>>>>>> 4ce53f2541db68d46cfaf9419f0cd50b06b35b63
                return response_message.content

        except Exception as e:
            return f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}"

    def _execute_tool_calls(self, tool_calls) -> List[Dict]:
<<<<<<< HEAD
        """æ‰§è¡Œå·¥å…·è°ƒç”¨å¹¶è¿”å›ç»“æœ"""
=======
        """æ‰§è¡Œå·¥å…·è°ƒç”¨å¹¶è¿”å›ç»“æœ (ä¿æŒåŸé€»è¾‘ä¸å˜)"""
>>>>>>> 4ce53f2541db68d46cfaf9419f0cd50b06b35b63
        tool_results = []

        for tool_call in tool_calls:
            tool_name = tool_call.function.name

<<<<<<< HEAD
            # è§£æå·¥å…·å‚æ•°ï¼Œå¢åŠ é”™è¯¯å¤„ç†
=======
>>>>>>> 4ce53f2541db68d46cfaf9419f0cd50b06b35b63
            try:
                if isinstance(tool_call.function.arguments, str):
                    tool_args = json.loads(tool_call.function.arguments)
                elif isinstance(tool_call.function.arguments, dict):
                    tool_args = tool_call.function.arguments
                else:
                    tool_args = {}
                    print(f"âš ï¸ å·¥å…·å‚æ•°æ ¼å¼å¼‚å¸¸: {type(tool_call.function.arguments)}")
            except json.JSONDecodeError as e:
                print(f"âŒ è§£æå·¥å…·å‚æ•°å¤±è´¥: {e}")
                tool_args = {}

            print(f"ğŸ”§ æ‰§è¡Œå·¥å…·: {tool_name} å‚æ•°: {tool_args}")

<<<<<<< HEAD
            # æ‰§è¡Œå·¥å…·
            tool_result = self.tool_manager.execute_tool(tool_name, tool_args)

            # ç‰¹æ®Šå¤„ç†å‡ºé¢˜å·¥å…·çš„ç»“æœ
            if tool_name == "quiz_generation" and isinstance(tool_result, dict) and "quiz_data" in tool_result:
                # å°†é¢˜ç›®æ•°æ®å­˜å‚¨åˆ°session_stateä¸­
=======
            tool_result = self.tool_manager.execute_tool(tool_name, tool_args)

            if tool_name == "quiz_generation" and isinstance(tool_result, dict) and "quiz_data" in tool_result:
>>>>>>> 4ce53f2541db68d46cfaf9419f0cd50b06b35b63
                try:
                    import streamlit as st
                    if not hasattr(st.session_state, 'generated_quiz'):
                        st.session_state.generated_quiz = []
                    st.session_state.generated_quiz.append(tool_result["quiz_data"])
                    print(f"ğŸ“š å·²å°† {len(tool_result['quiz_data']['questions'])} é“é¢˜ç›®å­˜å‚¨åˆ°UI")

<<<<<<< HEAD
                    # åŒæ—¶ä¿å­˜ä¹ é¢˜ç”Ÿæˆè®°å½•åˆ°å¯¹è¯å†å²
=======
>>>>>>> 4ce53f2541db68d46cfaf9419f0cd50b06b35b63
                    quiz_generation_record = {
                        "role": "assistant",
                        "content": f"ğŸ¯ å·²ç”Ÿæˆä¹ é¢˜ï¼š{tool_result['quiz_data']['topic']} - {len(tool_result['quiz_data']['questions'])}é“é¢˜ç›®",
                        "quiz_generation": {
                            "topic": tool_result["quiz_data"]["topic"],
                            "difficulty": tool_result["quiz_data"]["difficulty"],
                            "question_type": tool_result["quiz_data"]["question_type"],
                            "num_questions": len(tool_result["quiz_data"]["questions"]),
                            "questions": tool_result["quiz_data"]["questions"],
                            "timestamp": datetime.now().isoformat()
                        }
                    }

<<<<<<< HEAD
                    # æ·»åŠ åˆ°å¯¹è¯å†å²
=======
>>>>>>> 4ce53f2541db68d46cfaf9419f0cd50b06b35b63
                    if not hasattr(st.session_state, 'chat_history'):
                        st.session_state.chat_history = []
                    st.session_state.chat_history.append(quiz_generation_record)

                except ImportError:
<<<<<<< HEAD
                    # éStreamlitç¯å¢ƒï¼Œè·³è¿‡UIæ›´æ–°
                    pass

                # ä½¿ç”¨æ¶ˆæ¯éƒ¨åˆ†ä½œä¸ºå·¥å…·ç»“æœ
=======
                    pass

>>>>>>> 4ce53f2541db68d46cfaf9419f0cd50b06b35b63
                tool_content = tool_result["message"]
            else:
                tool_content = tool_result

<<<<<<< HEAD
            # æ ¼å¼åŒ–å·¥å…·ç»“æœæ¶ˆæ¯
=======
>>>>>>> 4ce53f2541db68d46cfaf9419f0cd50b06b35b63
            tool_result_message = {
                "role": "tool",
                "tool_call_id": tool_call.id,
                "content": tool_content
            }

            tool_results.append(tool_result_message)

        return tool_results

    def answer_question(
        self, query: str, chat_history: Optional[List[Dict]] = None, top_k: int = TOP_K
    ) -> str:
        """å›ç­”é—®é¢˜"""
        
        # ã€ä¿®æ”¹ã€‘ä¼ å…¥ chat_history ä»¥æ”¯æŒå¤šè½®æ£€ç´¢å¢å¼ºå’Œç­–ç•¥åˆ†æ´¾
        context, retrieved_docs = self.retrieve_context(query, chat_history=chat_history, top_k=top_k)

        if not context:
            context = "ï¼ˆæœªæ£€ç´¢åˆ°ç‰¹åˆ«ç›¸å…³çš„è¯¾ç¨‹ææ–™ï¼‰"

        answer = self.generate_response(query, context, chat_history)

        return answer

    def answer_image_question(
        self,
        query: str,
        image_base64: str,
        chat_history: Optional[List[Dict]] = None,
        top_k: int = TOP_K
    ) -> str:
<<<<<<< HEAD
        """å›ç­”åŒ…å«å›¾ç‰‡çš„é—®é¢˜

        å‚æ•°:
            query: ç”¨æˆ·å…³äºå›¾ç‰‡çš„é—®é¢˜
            image_base64: å›¾ç‰‡çš„Base64ç¼–ç 
            chat_history: å¯¹è¯å†å²
            top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡

        è¿”å›:
            ç”Ÿæˆçš„å›ç­”
        """
=======
        """å›ç­”åŒ…å«å›¾ç‰‡çš„é—®é¢˜"""
>>>>>>> 4ce53f2541db68d46cfaf9419f0cd50b06b35b63
        try:
            # 1. ä½¿ç”¨Qwen-VLåˆ†æå›¾ç‰‡ï¼Œç”Ÿæˆæ–‡å­—æè¿°
            print("ğŸ–¼ï¸ æ­£åœ¨åˆ†æå›¾ç‰‡...")
            image_description = self._analyze_image_with_vl(image_base64)

            if not image_description:
                return "âŒ å›¾ç‰‡åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥å›¾ç‰‡æ ¼å¼æˆ–é‡è¯•ã€‚"

            # 2. å°†å›¾ç‰‡æè¿°å’Œç”¨æˆ·é—®é¢˜åˆå¹¶ï¼Œæ„é€ æ–°çš„æŸ¥è¯¢
            enhanced_query = f"""
<<<<<<< HEAD
ã€ç”¨æˆ·æäº¤çš„å›¾ç‰‡åˆ†æç»“æœã€‘
{image_description}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{query}

è¯·åŸºäºç”¨æˆ·æäº¤çš„å›¾ç‰‡åˆ†æç»“æœå’Œç›¸å…³è¯¾ç¨‹èµ„æ–™ï¼Œä¸“ä¸šåœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
"""

            # 3. ä½¿ç”¨RAGæµç¨‹å›ç­”é—®é¢˜
            print("ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³è¯¾ç¨‹å†…å®¹...")
            context, retrieved_docs = self.retrieve_context(enhanced_query, top_k=top_k)
=======
            ã€ç”¨æˆ·æäº¤çš„å›¾ç‰‡åˆ†æç»“æœã€‘
            {image_description}

            ã€ç”¨æˆ·é—®é¢˜ã€‘
            {query}

            è¯·åŸºäºç”¨æˆ·æäº¤çš„å›¾ç‰‡åˆ†æç»“æœå’Œç›¸å…³è¯¾ç¨‹èµ„æ–™ï¼Œä¸“ä¸šåœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
            """

            # 3. ä½¿ç”¨RAGæµç¨‹å›ç­”é—®é¢˜
            print("ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³è¯¾ç¨‹å†…å®¹...")
            # ã€ä¿®æ”¹ã€‘ä¼ å…¥ chat_history ä»¥æ”¯æŒå¤šè½®æ£€ç´¢å¢å¼ºå’Œç­–ç•¥åˆ†æ´¾
            context, retrieved_docs = self.retrieve_context(enhanced_query, chat_history=chat_history, top_k=top_k)
>>>>>>> 4ce53f2541db68d46cfaf9419f0cd50b06b35b63

            if not context:
                context = "ï¼ˆæœªæ£€ç´¢åˆ°ç‰¹åˆ«ç›¸å…³çš„è¯¾ç¨‹ææ–™ï¼‰"

            # 4. ç”Ÿæˆæœ€ç»ˆå›ç­”
            print("ğŸ¤” æ­£åœ¨ç”Ÿæˆå›ç­”...")
            answer = self.generate_response(enhanced_query, context, chat_history)

            return answer

        except Exception as e:
            error_msg = f"å›¾ç‰‡é—®ç­”å¤„ç†å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            return f"âŒ {error_msg}"

    def _analyze_image_with_vl(self, image_base64: str) -> str:
<<<<<<< HEAD
        """ä½¿ç”¨Qwen-VLåˆ†æå›¾ç‰‡ï¼Œè¿”å›æ–‡å­—æè¿°"""
        try:
            # ç›´æ¥ä½¿ç”¨image_processorçš„analyze_single_imageæ–¹æ³•
            result = self.image_processor.analyze_single_image(image_base64, "ç”¨æˆ·ä¸Šä¼ å›¾ç‰‡")

            # æå–çº¯æè¿°å†…å®¹ï¼ˆå»æ‰æ ¼å¼åŒ–å‰ç¼€ï¼‰
            if result.startswith("--- ç”¨æˆ·ä¸Šä¼ å›¾ç‰‡ åˆ†æç»“æœ ---"):
                # å»æ‰æ ¼å¼åŒ–å‰ç¼€ï¼Œåªä¿ç•™åˆ†æç»“æœ
=======
        """ä½¿ç”¨Qwen-VLåˆ†æå›¾ç‰‡ï¼Œè¿”å›æ–‡å­—æè¿° (ä¿æŒåŸé€»è¾‘ä¸å˜)"""
        try:
            result = self.image_processor.analyze_single_image(image_base64, "ç”¨æˆ·ä¸Šä¼ å›¾ç‰‡")

            if result.startswith("--- ç”¨æˆ·ä¸Šä¼ å›¾ç‰‡ åˆ†æç»“æœ ---"):
>>>>>>> 4ce53f2541db68d46cfaf9419f0cd50b06b35b63
                lines = result.strip().split('\n')
                if len(lines) > 1:
                    return '\n'.join(lines[1:]).strip()

            return result.strip()

        except Exception as e:
            print(f"å›¾ç‰‡åˆ†æå¤±è´¥: {e}")
            return None

    def chat(self) -> None:
        """äº¤äº’å¼å¯¹è¯ (ä¿æŒåŸé€»è¾‘ä¸å˜)"""
        print("=" * 60)
        print("æ¬¢è¿ä½¿ç”¨æ™ºèƒ½è¯¾ç¨‹åŠ©æ•™ç³»ç»Ÿï¼")
        print("å½“å‰ RAG ç­–ç•¥æ¨¡å¼:", "é«˜çº§å¢å¼ºæ¨¡å¼" if self.enable_advanced_rag else "çº¯å‘é‡åŸºçº¿æ¨¡å¼ (DENSE)")
        print("=" * 60)

        chat_history = []

        while True:
            try:
                query = input("\nå­¦ç”Ÿ: ").strip()

                if not query:
                    continue

                answer = self.answer_question(query, chat_history=chat_history)

                print(f"\nåŠ©æ•™: {answer}")

                chat_history.append({"role": "user", "content": query})
                chat_history.append({"role": "assistant", "content": answer})

            except Exception as e:
                print(f"\né”™è¯¯: {str(e)}")