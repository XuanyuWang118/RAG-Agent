import os
import joblib
import uuid
from datetime import datetime
from typing import List, Dict, Any, Optional
from tqdm import tqdm

import chromadb
from chromadb.config import Settings
from openai import OpenAI

from langchain_core.documents import Document
from langchain_community.retrievers import BM25Retriever 

from config import (
    VECTOR_DB_PATH,
    COLLECTION_NAME,
    OPENAI_API_KEY,
    OPENAI_API_BASE,
    OPENAI_EMBEDDING_MODEL,
    TOP_K,
    RRF_K,
)

BM25_INDEX_PATH = os.path.join(VECTOR_DB_PATH, "bm25_index.joblib")

class VectorStore:

    def __init__(
        self,
        db_path: str = VECTOR_DB_PATH,
        collection_name: str = COLLECTION_NAME,
        api_key: str = OPENAI_API_KEY,
        api_base: str = OPENAI_API_BASE,
    ):
        self.db_path = db_path
        self.collection_name = collection_name

        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.client = OpenAI(api_key=api_key, base_url=api_base)

        # åˆå§‹åŒ–ChromaDB
        os.makedirs(db_path, exist_ok=True)
        self.chroma_client = chromadb.PersistentClient(
            path=db_path, settings=Settings(anonymized_telemetry=False)
        )

        # è·å–æˆ–åˆ›å»ºcollection
        self.collection = self.chroma_client.get_or_create_collection(
            name=collection_name, metadata={"description": "è¯¾ç¨‹ææ–™å‘é‡æ•°æ®åº“"}
        )
        
        # ã€ä¼˜åŒ– 1ï¼šå¼•å…¥ BM25 æ£€ç´¢å™¨ã€‘
        # BM25 æ£€ç´¢å™¨éœ€è¦æ‰€æœ‰æ–‡æ¡£æ‰èƒ½åˆå§‹åŒ–ï¼Œè¿™é‡Œå…ˆåˆå§‹åŒ–ä¸º Noneã€‚
        self.bm25_retriever: Optional[BM25Retriever] = None

        # ã€å¯åŠ¨æ—¶å°è¯•åŠ è½½ BM25 ç´¢å¼•ã€‘
        if os.path.exists(BM25_INDEX_PATH):
            print("ğŸš€ æ­£åœ¨åŠ è½½å·²å­˜åœ¨çš„ BM25 ç¨€ç–æ£€ç´¢å™¨...")
            try:
                # å°è¯•ä»ç£ç›˜åŠ è½½ç´¢å¼•
                self.bm25_retriever = joblib.load(BM25_INDEX_PATH)
                print("âœ… BM25 æ£€ç´¢å™¨åŠ è½½å®Œæˆã€‚")
            except Exception as e:
                print(f"âŒ BM25 ç´¢å¼•åŠ è½½å¤±è´¥: {e}")
                self.bm25_retriever = None
        else:
            print("âš ï¸ BM25 ç¨€ç–æ£€ç´¢å™¨æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€é€šè¿‡ add_documents åˆå§‹åŒ–ã€‚")


    def get_embedding(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º

        TODO: ä½¿ç”¨OpenAI APIè·å–æ–‡æœ¬çš„embeddingå‘é‡

        """
        try:
            # è°ƒç”¨ OpenAI API è·å– embedding
            response = self.client.embeddings.create(
                model=OPENAI_EMBEDDING_MODEL,
                input=text
            )
            # è¿”å›ç¬¬ä¸€ä¸ªï¼ˆä¹Ÿæ˜¯å”¯ä¸€çš„ï¼‰embedding å‘é‡
            return response.data[0].embedding
        except Exception as e:
            print(f"è·å– Embedding å¤±è´¥: {e}")
            return []

    def _initialize_bm25_retriever(self, lc_documents: List[Document]) -> None:
        """
        ã€ä¼˜åŒ– 2ï¼šæ–°å¢ BM25 åˆå§‹åŒ–æ–¹æ³•ã€‘
        ç§æœ‰æ–¹æ³•ï¼šä½¿ç”¨æ‰€æœ‰ LangChain Document å¯¹è±¡åˆå§‹åŒ– BM25 ç¨€ç–æ£€ç´¢å™¨ï¼Œå¹¶æŒä¹…åŒ–åˆ°ç£ç›˜ã€‚
        """
        if not lc_documents:
             print("âš ï¸ æ— æ³•åˆå§‹åŒ– BM25 æ£€ç´¢å™¨ï¼šæ²¡æœ‰æ–‡æ¡£ã€‚")
             return

        print("æ­£åœ¨åˆå§‹åŒ– BM25 ç¨€ç–æ£€ç´¢å™¨...")
        # ä»æ–‡æ¡£åˆ—è¡¨ä¸­åˆ›å»º BM25 ç´¢å¼•
        self.bm25_retriever = BM25Retriever.from_documents(lc_documents)
        self.bm25_retriever.k = TOP_K 
        print("âœ… BM25 æ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆã€‚")

        # ã€å°† BM25 æ£€ç´¢å™¨æŒä¹…åŒ–åˆ°ç£ç›˜ã€‘
        try:
            joblib.dump(self.bm25_retriever, BM25_INDEX_PATH)
            print(f"ğŸ’¾ BM25 æ£€ç´¢å™¨å·²æˆåŠŸä¿å­˜åˆ° {BM25_INDEX_PATH}")
        except Exception as e:
            print(f"âŒ è­¦å‘Šï¼šBM25 æ£€ç´¢å™¨æŒä¹…åŒ–å¤±è´¥: {e}")


    def add_documents(self, chunks: List[Dict[str, Any]]) -> None:
        """æ·»åŠ æ–‡æ¡£å—åˆ°å‘é‡æ•°æ®åº“
        TODO: å®ç°æ–‡æ¡£å—æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
        è¦æ±‚ï¼š
        1. éå†æ–‡æ¡£å—
        2. è·å–æ–‡æ¡£å—å†…å®¹
        3. è·å–æ–‡æ¡£å—å…ƒæ•°æ®
        5. æ‰“å°æ·»åŠ è¿›åº¦
        ã€ä¼˜åŒ– 3ï¼šæ”¹é€  add_documentsã€‘
        æ·»åŠ æ–‡æ¡£å—åˆ°å‘é‡æ•°æ®åº“ï¼Œå¹¶ç”¨äºåˆå§‹åŒ– BM25 ç´¢å¼•ã€‚
        """
        if not chunks:
            print("æ²¡æœ‰æ–‡æ¡£å—å¯ä»¥æ·»åŠ ã€‚")
            return

        texts = [chunk['content'] for chunk in chunks]
        metadatas = []
        ids = []
        lc_documents = [] # æ–°å¢ï¼šç”¨äº BM25 ç´¢å¼•çš„ LangChain æ–‡æ¡£åˆ—è¡¨

        # æ‰¹é‡è·å– Embedding
        print("æ­£åœ¨æ‰¹é‡è·å– Embedding å‘é‡...")
        embeddings = []
        for text in tqdm(texts, desc="è·å–å‘é‡"):
            embeddings.append(self.get_embedding(text))
        
        # å‡†å¤‡ ChromaDB æ•°æ®å’Œ LangChain Document åˆ—è¡¨
        for i, chunk in enumerate(chunks):
            # è·å–æ–‡æ¡£å—å…ƒæ•°æ®
            metadata = {
                "filename": chunk.get("filename"),
                "filetype": chunk.get("filetype"),
                "page_number": chunk.get("page_number"),
                "chunk_id": chunk.get("chunk_id"),
                "filepath": chunk.get("filepath"),
            }
            metadatas.append(metadata)

            # ä½¿ç”¨æ–‡ä»¶åã€é¡µç ã€å—IDç»„åˆç”Ÿæˆå”¯ä¸€ID
            unique_id = f"{chunk.get('filename')}_{chunk.get('page_number')}_{chunk.get('chunk_id')}_{i}"
            ids.append(unique_id)
            
            # åˆ›å»º LangChain Document å¯¹è±¡ (ç”¨äº BM25)
            lc_doc = Document(page_content=chunk['content'], metadata=metadata)
            lc_documents.append(lc_doc)


        # æ‰¹é‡æ·»åŠ æ–‡æ¡£å—åˆ° ChromaDB
        print("æ­£åœ¨æ‰¹é‡æ·»åŠ æ–‡æ¡£å—åˆ° ChromaDB...")
        try:
            self.collection.add(
                embeddings=embeddings,
                documents=texts,
                metadatas=metadatas,
                ids=ids
            )
            print(f"âœ… æˆåŠŸå°† {len(ids)} ä¸ªæ–‡æ¡£å—æ·»åŠ åˆ°å‘é‡æ•°æ®åº“ä¸­ã€‚")
            
            # åˆå§‹åŒ– BM25 æ£€ç´¢å™¨
            self._initialize_bm25_retriever(lc_documents)
            
        except Exception as e:
            print(f"âŒ æ·»åŠ æ–‡æ¡£åˆ° ChromaDB å¤±è´¥: {e}")


    def add_documents_incremental(self, chunks: List[Dict[str, str]]) -> bool:
        """å¢é‡æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“

        å‚æ•°:
            chunks: æ–‡æ¡£å—åˆ—è¡¨ï¼Œæ¯ä¸ªå—åŒ…å«contentå’Œmetadata

        è¿”å›:
            bool: æ˜¯å¦æ·»åŠ æˆåŠŸ
        """
        if not chunks:
            print("æ²¡æœ‰æ–‡æ¡£å—éœ€è¦æ·»åŠ ")
            return True

        try:
            # è·å–ç°æœ‰æ–‡æ¡£æ•°é‡ï¼Œç”¨äºç”Ÿæˆæ–°ID
            current_count = self.collection.count()

            texts = [chunk['content'] for chunk in chunks]
            metadatas = []
            ids = []
            new_lc_documents = [] # æ–°å¢ï¼šç”¨äºå¢é‡æ›´æ–° BM25

            # æ‰¹é‡è·å–embeddingsï¼ˆå¤ç”¨ç°æœ‰çš„get_embeddingæ–¹æ³•ï¼‰
            print(f"æ­£åœ¨å‘é‡åŒ– {len(texts)} ä¸ªæ–°æ–‡æ¡£å—...")
            embeddings = []
            for i, text in enumerate(texts):
                if (i + 1) % 10 == 0:  # æ¯10ä¸ªæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                    print(f"å·²å¤„ç† {i + 1}/{len(texts)} ä¸ªæ–‡æ¡£å—")
                embeddings.append(self.get_embedding(text))

            # å‡†å¤‡metadataå’ŒIDs
            for i, chunk in enumerate(chunks):
                metadata = {
                    "filename": chunk.get("filename", "unknown"),
                    "filetype": chunk.get("filetype", ""),
                    "page_number": chunk.get("page_number", 0),
                    "chunk_id": chunk.get("chunk_id", i),
                    "filepath": chunk.get("filepath", ""),
                    "added_at": datetime.now().isoformat(),  # æ ‡è®°æ·»åŠ æ—¶é—´
                    "added_incrementally": True  # æ ‡è®°ä¸ºå¢é‡æ·»åŠ 
                }
                metadatas.append(metadata)

                # ç”Ÿæˆå”¯ä¸€IDï¼ˆé¿å…ä¸ç°æœ‰IDå†²çªï¼‰
                unique_id = f"incremental_{current_count + i}_{uuid.uuid4().hex[:8]}"
                ids.append(unique_id)

            # æ‰¹é‡æ·»åŠ åˆ°ChromaDBï¼ˆå¤ç”¨ç°æœ‰çš„collectionæ“ä½œï¼‰
            print(f"æ­£åœ¨æ·»åŠ åˆ°å‘é‡æ•°æ®åº“...")
            self.collection.add(
                embeddings=embeddings,
                documents=texts,
                metadatas=metadatas,
                ids=ids
            )

            # ã€å¢é‡æ›´æ–° BM25 ç´¢å¼•ã€‘
            # å¢é‡æ·»åŠ åéœ€è¦é‡æ–°æ„å»ºæ•´ä¸ª BM25 ç´¢å¼•ï¼Œä»¥åŒ…å«æ–°æ–‡æ¡£
            print("ğŸ”„ å¢é‡æ·»åŠ å®Œæˆï¼Œæ­£åœ¨é‡å»º BM25 ç´¢å¼•...")
            
            # 1. ä» ChromaDB æ£€ç´¢æ‰€æœ‰ç°æœ‰æ–‡æ¡£
            all_chroma_docs = self.collection.get(
                include=['documents', 'metadatas']
            )
            
            # 2. å°†æ‰€æœ‰æ–‡æ¡£è½¬æ¢ä¸º LangChain Document æ ¼å¼
            all_lc_documents = [
                Document(page_content=all_chroma_docs['documents'][i], metadata=all_chroma_docs['metadatas'][i])
                for i in range(len(all_chroma_docs['documents']))
            ]
            
            # 3. ä½¿ç”¨æ‰€æœ‰æ–‡æ¡£é‡æ–°åˆå§‹åŒ– BM25 æ£€ç´¢å™¨
            self._initialize_bm25_retriever(all_lc_documents)
            
            print(f"âœ… å¢é‡æ·»åŠ æˆåŠŸï¼š{len(ids)} ä¸ªæ–‡æ¡£å—ï¼ŒBM25 ç´¢å¼•å·²é‡å»ºã€‚")
            return True

        except Exception as e:
            print(f"âŒ å¢é‡æ·»åŠ å¤±è´¥: {e}")
            return False

    def search_dense(self, query: str, top_k: int = TOP_K) -> List[Dict]:
        """æœç´¢ç›¸å…³æ–‡æ¡£

        TODO: å®ç°å‘é‡ç›¸ä¼¼åº¦æœç´¢
        è¦æ±‚ï¼š
        1. é¦–å…ˆè·å–æŸ¥è¯¢æ–‡æœ¬çš„embeddingå‘é‡ï¼ˆè°ƒç”¨self.get_embeddingï¼‰
        2. ä½¿ç”¨self.collectionè¿›è¡Œå‘é‡æœç´¢, å¾—åˆ°top_kä¸ªç»“æœ
        3. æ ¼å¼åŒ–è¿”å›ç»“æœï¼Œæ¯ä¸ªç»“æœåŒ…å«ï¼š
           - content: æ–‡æ¡£å†…å®¹
           - metadata: å…ƒæ•°æ®ï¼ˆæ–‡ä»¶åã€é¡µç ç­‰ï¼‰
        4. è¿”å›æ ¼å¼åŒ–çš„ç»“æœåˆ—è¡¨
        """

        # 1. è·å–æŸ¥è¯¢æ–‡æœ¬çš„ embedding å‘é‡
        query_embedding = self.get_embedding(query)
        
        if not query_embedding:
            return []

        # 2. ä½¿ç”¨ self.collection è¿›è¡Œå‘é‡æœç´¢
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k,
            include=['documents', 'metadatas', 'distances'] # åŒ…å«æ–‡æ¡£å†…å®¹å’Œå…ƒæ•°æ®
        )

        # 3. æ ¼å¼åŒ–è¿”å›ç»“æœ
        formatted_results = []
        
        # results ç»“æ„é€šå¸¸æ˜¯åµŒå¥—åˆ—è¡¨ï¼Œæˆ‘ä»¬æå–ç¬¬ä¸€ä¸ªç»“æœé›†
        if results and results.get("documents"):
            
            documents = results.get("documents", [[]])[0]
            metadatas = results.get("metadatas", [[]])[0]
            distances = results.get("distances", [[]])[0]
            
            for doc, meta, dist in zip(documents, metadatas, distances):
                formatted_results.append({
                    "content": doc,
                    "metadata": meta,
                    "distance": dist # å¯ä»¥ç”¨äºè°ƒè¯•æˆ–æ’åº
                })

        # 4. è¿”å›æ ¼å¼åŒ–çš„ç»“æœåˆ—è¡¨
        return formatted_results

    def search_bm25(self, query: str, top_k: int = TOP_K) -> List[Dict]:
        """
        ã€æ–°å¢/è¾…åŠ©æ–¹æ³•ã€‘å®ç°çº¯ç²¹çš„ BM25 ç¨€ç–æ£€ç´¢ã€‚
        æ³¨æ„ï¼šè¯¥æ–¹æ³•ä»…ä¾›å†…éƒ¨ä½¿ç”¨æˆ– RRF èåˆè°ƒç”¨ã€‚
        """
        if self.bm25_retriever is None:
            print("âš ï¸ è­¦å‘Š: BM25 æ£€ç´¢å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ‰§è¡Œç¨€ç–æ£€ç´¢ã€‚")
            return []

        # BM25 è¿”å› LangChain Document å¯¹è±¡ï¼Œä½¿ç”¨ invoke() æ–¹æ³•è¿›è¡Œæ£€ç´¢
        self.bm25_retriever.k = top_k # ç¡®ä¿ä½¿ç”¨ä¼ å…¥çš„ top_k
        bm25_docs = self.bm25_retriever.invoke(query)
        
        # æ ¼å¼åŒ– LangChain Document ä¸º List[Dict] æ ¼å¼
        formatted_results = []
        for doc in bm25_docs:
            formatted_results.append({
                "content": doc.page_content,
                "metadata": dict(doc.metadata),
                "distance": 0.0 # BM25 æœ¬èº«ä¸æä¾›è·ç¦»æˆ–ç›¸å…³æ€§åˆ†æ•°ï¼Œè®¾ä¸º 0.0
            })
        return formatted_results

    def search(self, query: str, top_k: int = TOP_K) -> List[Dict]:
        """
        ã€ä¼˜åŒ– 4ï¼šå®ç°æ··åˆæ£€ç´¢ (RRF èåˆ)ã€‘ ç»“åˆç¨€ç–æ£€ç´¢å’Œå¯†é›†æ£€ç´¢çš„ç»“æœï¼Œä½¿ç”¨ RRF ç®—æ³•é‡æ–°æ’åºã€‚
        """
        # æ£€æŸ¥ BM25 æ˜¯å¦å·²åˆå§‹åŒ–ï¼Œå¦‚æœæœªåˆå§‹åŒ–åˆ™é€€åŒ–ä¸ºçº¯å‘é‡æœç´¢
        if self.bm25_retriever is None:
            print("âš ï¸ è­¦å‘Š: æ­£åœ¨è¿›è¡Œçº¯å‘é‡æœç´¢ï¼ŒBM25 æ£€ç´¢å™¨æœªåˆå§‹åŒ–ã€‚")
            return self.search_dense(query, top_k=top_k)

        # 1. å¯†é›†æ£€ç´¢ (å‘é‡æœç´¢) - è·å– Top_K * 2 çš„ç»“æœï¼Œç•™ç»™ RRF èåˆ
        dense_results = self.search_dense(query, top_k=top_k * 2) 
        
        # 2. ç¨€ç–æ£€ç´¢ (BM25 å…³é”®è¯æœç´¢) - è·å– Top_K * 2 çš„ç»“æœ
        # BM25 è¿”å› LangChain Document å¯¹è±¡ï¼Œä½¿ç”¨ invoke() æ–¹æ³•è¿›è¡Œæ£€ç´¢
        bm25_docs = self.bm25_retriever.invoke(query)
        
        # 3. èåˆ (Reciprocal Rank Fusion, RRF) 
        fused_scores = {}
        all_results_map = {} # ç”¨äºå­˜å‚¨æ‰€æœ‰ç‹¬ç‰¹çš„æ–‡æ¡£å—ï¼Œæ–¹ä¾¿æŸ¥æ‰¾

        # è¾…åŠ©å‡½æ•°ï¼šæ ¹æ®æ–‡ä»¶åã€é¡µç å’Œå— ID åˆ›å»ºå”¯ä¸€é”®ï¼Œç”¨äºèåˆ
        def _get_unique_key(item: Dict[str, Any]) -> str:
            meta = item.get('metadata', {})
            # è¿™é‡Œçš„é”®å¿…é¡»ä¸ add_documents ä¸­çš„ ID ç”Ÿæˆé€»è¾‘ä¸€è‡´
            return f"{meta.get('filename')}_{meta.get('page_number')}_{meta.get('chunk_id')}"
        
        def _get_doc_key(doc: Document) -> str:
             return f"{doc.metadata.get('filename')}_{doc.metadata.get('page_number')}_{doc.metadata.get('chunk_id')}"

        # A. å¤„ç†å¯†é›†æ£€ç´¢ç»“æœ
        for i, item in enumerate(dense_results):
            key = _get_unique_key(item)
            rank = i + 1
            score = 1 / (RRF_K + rank)
            fused_scores[key] = fused_scores.get(key, 0) + score
            all_results_map[key] = item
            
        # B. å¤„ç†ç¨€ç–æ£€ç´¢ç»“æœ
        for i, doc in enumerate(bm25_docs):
            key = _get_doc_key(doc)
            rank = i + 1
            score = 1 / (RRF_K + rank)
            fused_scores[key] = fused_scores.get(key, 0) + score
            
            # å¦‚æœæ˜¯ BM25 ç‹¬æœ‰çš„ç»“æœï¼Œå°†å…¶åŠ å…¥ map ä¸­ï¼ˆéœ€è½¬æ¢ä¸º Dict æ ¼å¼ï¼‰
            if key not in all_results_map:
                 all_results_map[key] = {
                    "content": doc.page_content,
                    "metadata": dict(doc.metadata),
                    "distance": 0.0 # RRF èåˆåè·ç¦»ä¸å†æœ‰æ„ä¹‰
                }

        # 4. æ’åºå’Œæå– Top-K ç»“æœ
        
        # æ ¹æ®èåˆå¾—åˆ†é™åºæ’åˆ—æ‰€æœ‰å”¯ä¸€çš„æ–‡æ¡£é”®
        sorted_keys = sorted(fused_scores.keys(), key=lambda x: fused_scores[x], reverse=True)
        
        # æå– Top-K ç»“æœ
        final_results = []
        for key in sorted_keys:
            if len(final_results) >= top_k:
                break
            final_results.append(all_results_map[key])
            
        return final_results
    
    def clear_collection(self) -> None:
        """æ¸…ç©ºcollection"""
        self.chroma_client.delete_collection(name=self.collection_name)
        self.collection = self.chroma_client.create_collection(
            name=self.collection_name, metadata={"description": "è¯¾ç¨‹å‘é‡æ•°æ®åº“"}
        )
        print("å‘é‡æ•°æ®åº“å·²æ¸…ç©º")

    def get_collection_count(self) -> int:
        """è·å–collectionä¸­çš„æ–‡æ¡£æ•°é‡"""
        return self.collection.count()
