#!/usr/bin/env python3
"""
æµ‹è¯•ç½‘é¡µå¿«ç…§åŠŸèƒ½
"""

import requests
from bs4 import BeautifulSoup

def test_web_scraping():
    """æµ‹è¯•ç½‘é¡µæŠ“å–åŠŸèƒ½"""

    test_url = "https://example.com"

    print("ğŸ§ª æµ‹è¯•ç½‘é¡µæŠ“å–åŠŸèƒ½")
    print("=" * 40)

    try:
        # è®¾ç½®è¯·æ±‚å¤´
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

        print(f"ğŸŒ æ­£åœ¨æŠ“å–: {test_url}")

        # å‘é€è¯·æ±‚
        response = requests.get(test_url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = response.apparent_encoding

        print(f"âœ… HTTPçŠ¶æ€ç : {response.status_code}")
        print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(response.content)} bytes")

        # è§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')

        # æå–æ ‡é¢˜
        title = soup.title.string if soup.title else "æ— æ ‡é¢˜"
        print(f"ğŸ“„ ç½‘é¡µæ ‡é¢˜: {title}")

        # æ¸…ç†å†…å®¹
        for script in soup(["script", "style", "nav", "header", "footer", "aside"]):
            script.decompose()

        main_content = soup.get_text(separator='\n', strip=True)
        lines = [line.strip() for line in main_content.split('\n') if line.strip()]
        cleaned_content = '\n'.join(lines)

        print(f"ğŸ“ æå–çš„æ–‡æœ¬é•¿åº¦: {len(cleaned_content)} å­—ç¬¦")
        print(f"ğŸ“„ æ–‡æœ¬é¢„è§ˆ: {cleaned_content[:200]}...")

        print("\nâœ… ç½‘é¡µæŠ“å–æµ‹è¯•æˆåŠŸï¼")
        return True

    except Exception as e:
        print(f"âŒ ç½‘é¡µæŠ“å–æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    success = test_web_scraping()
    if success:
        print("\nğŸ‰ ç½‘é¡µå¿«ç…§åŠŸèƒ½å¯ä»¥æ­£å¸¸å·¥ä½œï¼")
    else:
        print("\nâš ï¸ ç½‘é¡µå¿«ç…§åŠŸèƒ½å¯èƒ½å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä¾èµ–åº“ã€‚")
