# RAG-Agent

NLP Course Project [Info](https://gpy5q03kes.feishu.cn/wiki/JgqTwaqG2ih6hdkWk6pcYBhAnPd)

Members: XuanyuWang, RuiTao, BoruiZhang

### Git usage
```bash
git pull                # 拉取仓库
git status              # 查看修改状态
git add .               # 把修改加入暂存区
git commit -m "message" # 提交到 github 仓库
git push                # 上传
```

### Data
NLP Course PPts


### Chat Example
```
============================================================
欢迎使用智能课程助教系统！
============================================================

学生: 解释一下对于大语言模型ChatGPT为什么效果这么好？

助教: ChatGPT之所以效果出色，主要归因于以下几个关键因素：

1. **预训练使大模型储存了大量知识**：通过在海量文本数据上进行预训练，ChatGPT学习并内化了广泛的世界知识，为其后续任务表现奠定了基础。[来源：4.2 大语言模型 v2.pptx, 幻灯片 39]

2. **指令微调提升了泛化与对齐能力**：在预训练基础上，通过指令微调（Instruction Tuning），模型学会了更好地理解和遵循人类指令，从而在多样化的任务中展现出强大的泛化能力，并实现与人类意图的对齐。[来源：4.2 大语言模型 v2.pptx, 幻灯片 39 和 幻灯片 86]

3. **代码数据增强了复杂推理能力**：训练数据中包含大量编程代码，这使得模型能够学习到结构化、逻辑严密的推理模式。其中，面向过程的编程模拟人类逐步解决问题的方式，而面向对象的编程则帮助模型将复杂任务分解为多个简单子任务，从而提升其处理复杂问题的能力。[来源：4.2 大语言模型 v2.pptx, 幻灯片 39]

4. **模型规模的指数级增长带来性能跃升与“涌现”现象**：随着参数量和训练数据规模的大幅增加，模型不仅在常规任务上表现更优，还在某些高阶能力（如复杂推理）上出现“涌现”——即在规模突破某一阈值后，能力突然显著提升。[来源：4.2 大语言模型 v2.pptx, 幻灯片 4 和 幻灯片 86]

5. **更智能的交互体验与通用性**：ChatGPT支持自然、流畅的人机对话，并能适应多种任务场景（如问答、创作、推理等），这种通用性和人性化交互是其广受欢迎的重要原因。[来源：4.2 大语言模型 v2.pptx, 幻灯片 3]

综上，ChatGPT的成功是大规模预训练、高质量指令微调、多样化训练数据（包括代码）、模型规模效应以及人机对齐设计共同作用的结果。
```
```
学生: 解释一下双向自编码模型BERT的优缺点？

助教: 双向自编码模型 BERT 的优缺点如下：

**优点：**  
- 在自然语言理解（NLU）任务上表现突出。  
- 使用基于 Attention 的 Transformer Encoder 结构，能够有效建模双向上下文信息，克服了传统自回归模型难以捕捉双向语境的局限性。

**缺点：**  
- 难以解决自然语言生成（NLG）问题，因其采用掩蔽语言模型（MLM）进行预训练，不适合逐词生成任务。  
- 预训练和微调阶段的输入空间分布不匹配（例如，预训练中使用 [MASK] 标记，而微调时通常没有）。  
- 预训练过程收敛较慢，且不能掩蔽过多词元，否则会影响训练效率和效果。  
- 假设被掩蔽的词元之间是独立的，这导致对联合概率的估计是有偏的。例如，在句子 “New York is a city.” 中，若 “New” 和 “York” 同时被掩蔽，模型无法利用它们之间的依赖关系进行准确预测。

[来源：4.1 预训练语言模型.pptx, 幻灯片 41、42]
```
```
学生: 分析一下当前俄乌战争局势？

助教: 我无法根据当前课程材料回答这个问题，请参考相关教材或联系老师。
```

### 添加
一个简单的functioncalling架构。

利用tavily的联网查询功能。

稍微修改了一下提示词来使用函数。
